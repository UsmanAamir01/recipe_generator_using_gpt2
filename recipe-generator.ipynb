{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13577548,"sourceType":"datasetVersion","datasetId":8625651}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T11:55:02.143283Z","iopub.execute_input":"2025-11-01T11:55:02.143529Z","iopub.status.idle":"2025-11-01T11:55:02.413563Z","shell.execute_reply.started":"2025-11-01T11:55:02.143509Z","shell.execute_reply":"2025-11-01T11:55:02.412914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers rouge-score nltk evaluate -q\n\nprint(\"‚úì Libraries installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:09:29.945116Z","iopub.execute_input":"2025-11-01T12:09:29.945474Z","iopub.status.idle":"2025-11-01T12:09:33.316085Z","shell.execute_reply.started":"2025-11-01T12:09:29.945448Z","shell.execute_reply":"2025-11-01T12:09:33.315144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom rouge_score import rouge_scorer\nimport re\nfrom tqdm import tqdm\nimport warnings\nimport nltk\n\n# Download NLTK data\nnltk.download('punkt', quiet=True)\n\nwarnings.filterwarnings('ignore')\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:14:16.973359Z","iopub.execute_input":"2025-11-01T12:14:16.974092Z","iopub.status.idle":"2025-11-01T12:14:18.200236Z","shell.execute_reply.started":"2025-11-01T12:14:16.974068Z","shell.execute_reply":"2025-11-01T12:14:18.199572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset\ncsv_path = '/kaggle/input/3a2m-cooking-recipe-dataset/3A2M_EXTENDED.csv'\ndf = pd.read_csv(csv_path)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nColumns: {df.columns.tolist()}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head(3))\n\n# Check for missing values\nprint(f\"\\nMissing values:\")\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:14:34.044456Z","iopub.execute_input":"2025-11-01T12:14:34.045046Z","iopub.status.idle":"2025-11-01T12:15:17.543760Z","shell.execute_reply.started":"2025-11-01T12:14:34.045024Z","shell.execute_reply":"2025-11-01T12:15:17.543112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean the data\nprint(\"Cleaning data...\")\ndf_clean = df.copy()\n\n# Drop rows with missing critical columns\n# Using 'NER' or 'Extended_NER' as ingredients column\ndf_clean = df_clean.dropna(subset=['title', 'directions'])\n\n# Clean title (remove \\t)\ndf_clean['title'] = df_clean['title'].str.strip()\n\n# Use Extended_NER as ingredients (it has more detail)\ndf_clean['ingredients'] = df_clean['Extended_NER']\n\n# Filter out very short entries\ndf_clean = df_clean[df_clean['ingredients'].str.len() > 10]\ndf_clean = df_clean[df_clean['directions'].str.len() > 20]\n\n# Reset index\ndf_clean = df_clean.reset_index(drop=True)\n\nprint(f\"\\nOriginal dataset size: {len(df)}\")\nprint(f\"Cleaned dataset size: {len(df_clean)}\")\n\n# ============================================================================\n# SPEED UP OPTION: Sample a subset for faster training\n# ============================================================================\n# RECOMMENDED: Use 50,000-100,000 samples for faster training with good results\n# For even faster experimentation, use 10,000-20,000 samples\n\nSAMPLE_SIZE = 50000  # Change this number based on your needs\n# Options: 10000 (very fast), 50000 (balanced), 100000 (slower but better), None (full dataset - very slow)\n\nif SAMPLE_SIZE and SAMPLE_SIZE < len(df_clean):\n    print(f\"\\n‚ö° SPEED UP: Using {SAMPLE_SIZE:,} samples instead of full dataset\")\n    df_clean = df_clean.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n    print(f\"This will train {len(df) // SAMPLE_SIZE:.0f}x faster!\")\nelse:\n    print(f\"\\n‚ö†Ô∏è Using full dataset ({len(df_clean):,} samples) - This will take many hours!\")\n\nprint(f\"Final dataset size: {len(df_clean):,}\")\n\n# Show examples\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAMPLE RECIPE:\")\nprint(\"=\"*80)\nsample = df_clean.iloc[0]\nprint(f\"Title: {sample['title']}\")\nprint(f\"\\nIngredients: {sample['ingredients'][:200]}...\")\nprint(f\"\\nDirections: {sample['directions'][:200]}...\")\nprint(f\"\\nGenre: {sample['genre']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:36:05.531580Z","iopub.execute_input":"2025-11-01T12:36:05.531852Z","iopub.status.idle":"2025-11-01T12:36:11.047138Z","shell.execute_reply.started":"2025-11-01T12:36:05.531832Z","shell.execute_reply":"2025-11-01T12:36:11.046393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RecipeDataset(Dataset):\n    \"\"\"Custom Dataset for recipe generation\"\"\"\n    \n    def __init__(self, data, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.examples = []\n        \n        print(f\"Processing {len(data)} recipes...\")\n        for _, row in tqdm(data.iterrows(), total=len(data)):\n            # Format: [INGREDIENTS] ingredients [RECIPE] title: steps\n            ingredients = str(row.get('ingredients', '')).strip()\n            title = str(row.get('title', '')).strip()\n            directions = str(row.get('directions', '')).strip()\n            \n            # Clean up list formatting from ingredients if it's a string representation of a list\n            if ingredients.startswith('[') and ingredients.endswith(']'):\n                # Remove brackets and quotes, clean up\n                ingredients = ingredients.strip('[]').replace(\"'\", \"\").replace('\"', '')\n            \n            # Clean up list formatting from directions if it's a string representation of a list\n            if directions.startswith('[') and directions.endswith(']'):\n                directions = directions.strip('[]').replace('\", \"', ' ').replace('\",\"', ' ').replace('\"', '').replace(\"'\", \"\")\n            \n            if ingredients and title and directions:\n                formatted_text = f\"[INGREDIENTS] {ingredients} [RECIPE] {title}: {directions}<|endoftext|>\"\n                self.examples.append(formatted_text)\n        \n        print(f\"Created {len(self.examples)} training examples\")\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        text = self.examples[idx]\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        input_ids = encodings['input_ids'].squeeze()\n        attention_mask = encodings['attention_mask'].squeeze()\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids.clone()\n        }\n\nprint(\"‚úì RecipeDataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:37:32.382545Z","iopub.execute_input":"2025-11-01T12:37:32.382819Z","iopub.status.idle":"2025-11-01T12:37:32.391073Z","shell.execute_reply.started":"2025-11-01T12:37:32.382800Z","shell.execute_reply":"2025-11-01T12:37:32.390307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading GPT-2 model and tokenizer...\")\n\n# Load tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel = model.to(device)\n\nprint(f\"‚úì Model loaded with {model.num_parameters():,} parameters\")\nprint(f\"‚úì Tokenizer vocab size: {len(tokenizer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:37:38.271503Z","iopub.execute_input":"2025-11-01T12:37:38.271772Z","iopub.status.idle":"2025-11-01T12:37:39.079351Z","shell.execute_reply.started":"2025-11-01T12:37:38.271754Z","shell.execute_reply":"2025-11-01T12:37:39.078703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\ntrain_split = 0.9\ntrain_size = int(len(df_clean) * train_split)\n\ntrain_df = df_clean[:train_size].reset_index(drop=True)\nval_df = df_clean[train_size:].reset_index(drop=True)\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\n\n# Create datasets\nprint(\"\\nCreating training dataset...\")\ntrain_dataset = RecipeDataset(train_df, tokenizer, max_length=512)\n\nprint(\"\\nCreating validation dataset...\")\nval_dataset = RecipeDataset(val_df, tokenizer, max_length=512)\n\n# Create dataloaders\nbatch_size = 4  # Adjust based on GPU memory\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\n‚úì Training batches: {len(train_loader)}\")\nprint(f\"‚úì Validation batches: {len(val_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:38:09.730341Z","iopub.execute_input":"2025-11-01T12:38:09.730609Z","iopub.status.idle":"2025-11-01T12:38:12.178709Z","shell.execute_reply.started":"2025-11-01T12:38:09.730589Z","shell.execute_reply":"2025-11-01T12:38:12.177884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 3\nlearning_rate = 5e-5\nwarmup_steps = 500\nmax_grad_norm = 1.0\n\n# SPEED UP OPTIONS:\n# 1. Increase batch size if you have GPU memory (4 -> 8 or 16)\n# 2. Reduce epochs (3 -> 2 or even 1 for quick testing)\n# 3. Use gradient accumulation for effective larger batch size\n\nbatch_size = 8  # Increase to 16 if GPU allows (faster training)\ngradient_accumulation_steps = 2  # Effective batch size = batch_size * this\n\nprint(\"=\"*80)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\"*80)\nprint(f\"Dataset size: {len(train_dataset):,} samples\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Gradient accumulation: {gradient_accumulation_steps}\")\nprint(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\nprint(f\"Epochs: {epochs}\")\nprint(f\"Learning rate: {learning_rate}\")\nprint(f\"Device: {device}\")\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n# Learning rate scheduler\ntotal_steps = (len(train_loader) // gradient_accumulation_steps) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\nprint(f\"Total training steps: {total_steps:,}\")\nprint(f\"Warmup steps: {warmup_steps}\")\n\n# Estimate training time\nsamples_per_sec = 5 if device == 'cuda' else 0.5  # Rough estimates\ntotal_time_mins = (len(train_dataset) * epochs) / (samples_per_sec * 60)\nprint(f\"\\n‚è±Ô∏è Estimated training time: {total_time_mins:.1f} minutes ({total_time_mins/60:.1f} hours)\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:38:30.926030Z","iopub.execute_input":"2025-11-01T12:38:30.926342Z","iopub.status.idle":"2025-11-01T12:38:30.934873Z","shell.execute_reply.started":"2025-11-01T12:38:30.926321Z","shell.execute_reply":"2025-11-01T12:38:30.933948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, scheduler, device, gradient_accumulation_steps=1):\n    \"\"\"Train for one epoch with gradient accumulation\"\"\"\n    model.train()\n    total_loss = 0\n    optimizer.zero_grad()\n    \n    progress_bar = tqdm(train_loader, desc=\"Training\")\n    for idx, batch in enumerate(progress_bar):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss / gradient_accumulation_steps  # Scale loss\n        total_loss += loss.item() * gradient_accumulation_steps\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights every gradient_accumulation_steps\n        if (idx + 1) % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        progress_bar.set_postfix({'loss': f'{loss.item() * gradient_accumulation_steps:.4f}'})\n    \n    avg_loss = total_loss / len(train_loader)\n    return avg_loss\n\ndef validate(model, val_loader, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            total_loss += outputs.loss.item()\n    \n    avg_loss = total_loss / len(val_loader)\n    return avg_loss\n\nprint(\"‚úì Training functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:38:53.051556Z","iopub.execute_input":"2025-11-01T12:38:53.051826Z","iopub.status.idle":"2025-11-01T12:38:53.060660Z","shell.execute_reply.started":"2025-11-01T12:38:53.051805Z","shell.execute_reply":"2025-11-01T12:38:53.059863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\ntraining_stats = []\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*70)\n\nfor epoch in range(epochs):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, gradient_accumulation_steps)\n    \n    # Validation\n    val_loss = validate(model, val_loader, device)\n    \n    print(f\"\\nüìä Epoch {epoch + 1} Results:\")\n    print(f\"   Average train loss: {train_loss:.4f}\")\n    print(f\"   Average val loss: {val_loss:.4f}\")\n    \n    training_stats.append({\n        'epoch': epoch + 1,\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    })\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        model.save_pretrained('./best_recipe_model')\n        tokenizer.save_pretrained('./best_recipe_model')\n        print(f\"   ‚úì Saved best model (val_loss: {val_loss:.4f})\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\n\n# Show training stats\nstats_df = pd.DataFrame(training_stats)\nprint(\"\\n\", stats_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T12:39:05.330815Z","iopub.execute_input":"2025-11-01T12:39:05.331583Z","iopub.status.idle":"2025-11-01T12:39:22.220909Z","shell.execute_reply.started":"2025-11-01T12:39:05.331557Z","shell.execute_reply":"2025-11-01T12:39:22.219926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RecipeGenerator:\n    \"\"\"Generate and evaluate recipes\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.model.eval()\n    \n    def generate_from_ingredients(self, ingredients, title=\"\", max_length=300, \n                                  temperature=0.8, top_p=0.9, num_return=1):\n        \"\"\"Generate recipe from ingredients\"\"\"\n        if title:\n            prompt = f\"[INGREDIENTS] {ingredients} [RECIPE] {title}:\"\n        else:\n            prompt = f\"[INGREDIENTS] {ingredients} [RECIPE]\"\n        \n        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n        \n        with torch.no_grad():\n            output = self.model.generate(\n                input_ids,\n                max_length=max_length,\n                num_return_sequences=num_return,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                no_repeat_ngram_size=2\n            )\n        \n        recipes = []\n        for seq in output:\n            text = self.tokenizer.decode(seq, skip_special_tokens=True)\n            recipes.append(self._format_output(text))\n        \n        return recipes if num_return > 1 else recipes[0]\n    \n    def generate_from_title(self, title, max_length=300, temperature=0.8, top_p=0.9):\n        \"\"\"Generate recipe from title only\"\"\"\n        prompt = f\"[RECIPE] {title}:\"\n        \n        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n        \n        with torch.no_grad():\n            output = self.model.generate(\n                input_ids,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id,\n                no_repeat_ngram_size=2\n            )\n        \n        text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n        return self._format_output(text)\n    \n    def _format_output(self, text):\n        \"\"\"Format generated text into readable recipe\"\"\"\n        # Extract recipe part\n        if '[RECIPE]' in text:\n            parts = text.split('[RECIPE]')\n            recipe_part = parts[1].strip()\n            return recipe_part\n        return text\n\nprint(\"‚úì RecipeGenerator class defined\")\n\n# Initialize generator\ngenerator = RecipeGenerator(model, tokenizer, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"EXAMPLE RECIPE GENERATIONS\")\nprint(\"=\"*80)\n\n# Example 1: From ingredients with title\nprint(\"\\n\" + \"‚îÄ\"*80)\nprint(\"Example 1: Generate from ingredients + title\")\nprint(\"‚îÄ\"*80)\ningredients_1 = \"chicken breast, garlic, olive oil, lemon juice, rosemary, salt, pepper\"\ntitle_1 = \"Roasted Lemon Chicken\"\nprint(f\"ü•ò Ingredients: {ingredients_1}\")\nprint(f\"üìù Title: {title_1}\")\nprint(\"\\nüìñ Generated Recipe:\")\nrecipe_1 = generator.generate_from_ingredients(ingredients_1, title_1, temperature=0.8)\nprint(recipe_1)\n\n# Example 2: From ingredients without title\nprint(\"\\n\" + \"‚îÄ\"*80)\nprint(\"Example 2: Generate from ingredients only\")\nprint(\"‚îÄ\"*80)\ningredients_2 = \"flour, butter, sugar, eggs, vanilla extract, baking powder, milk\"\nprint(f\"ü•ò Ingredients: {ingredients_2}\")\nprint(\"\\nüìñ Generated Recipe:\")\nrecipe_2 = generator.generate_from_ingredients(ingredients_2, temperature=0.9)\nprint(recipe_2)\n\n# Example 3: From title only\nprint(\"\\n\" + \"‚îÄ\"*80)\nprint(\"Example 3: Generate from title only\")\nprint(\"‚îÄ\"*80)\ntitle_3 = \"Chocolate Chip Cookies\"\nprint(f\"üìù Title: {title_3}\")\nprint(\"\\nüìñ Generated Recipe:\")\nrecipe_3 = generator.generate_from_title(title_3, temperature=0.8)\nprint(recipe_3)\n\n# Example 4: Creative generation\nprint(\"\\n\" + \"‚îÄ\"*80)\nprint(\"Example 4: Creative recipe with higher temperature\")\nprint(\"‚îÄ\"*80)\ningredients_4 = \"salmon, honey, soy sauce, ginger\"\ntitle_4 = \"Honey Glazed Salmon\"\nprint(f\"ü•ò Ingredients: {ingredients_4}\")\nprint(f\"üìù Title: {title_4}\")\nprint(\"\\nüìñ Generated Recipe:\")\nrecipe_4 = generator.generate_from_ingredients(ingredients_4, title_4, temperature=1.0)\nprint(recipe_4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_rouge_scores(predictions, references):\n    \"\"\"Calculate ROUGE scores using rouge_score library\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    \n    for pred, ref in zip(predictions, references):\n        scores = scorer.score(ref, pred)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n    \n    return {\n        'rouge1': np.mean(rouge1_scores),\n        'rouge2': np.mean(rouge2_scores),\n        'rougeL': np.mean(rougeL_scores),\n    }\n\ndef calculate_bleu_score(predictions, references):\n    \"\"\"Calculate BLEU scores\"\"\"\n    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n    smoothie = SmoothingFunction().method4\n    \n    bleu_scores = []\n    for pred, ref in zip(predictions, references):\n        pred_tokens = pred.split()\n        ref_tokens = [ref.split()]\n        score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothie)\n        bleu_scores.append(score)\n    \n    return np.mean(bleu_scores)\n\nprint(\"‚úì Evaluation functions defined\")\n\n\n\nprint(\"=\"*80)\nprint(\"QUALITY EVALUATION\")\nprint(\"=\"*80)\n\n# Sample validation examples for evaluation\nnum_eval_samples = 20\neval_samples = val_df.head(num_eval_samples)\n\ngenerated_recipes = []\nreference_recipes = []\n\nprint(f\"\\nGenerating {num_eval_samples} recipes for evaluation...\")\n\nfor idx, row in tqdm(eval_samples.iterrows(), total=num_eval_samples):\n    ingredients = str(row['ingredients'])\n    title = str(row['title'])\n    reference = str(row['directions'])\n    \n    # Generate recipe\n    generated = generator.generate_from_ingredients(\n        ingredients, \n        title, \n        temperature=0.8,\n        max_length=300\n    )\n    \n    generated_recipes.append(generated)\n    reference_recipes.append(reference)\n\n# Calculate metrics\nprint(\"\\nCalculating ROUGE scores...\")\nrouge_scores = calculate_rouge_scores(generated_recipes, reference_recipes)\n\nprint(\"\\nCalculating BLEU score...\")\nbleu_score = calculate_bleu_score(generated_recipes, reference_recipes)\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*80)\nprint(f\"\\nüìä ROUGE Scores:\")\nprint(f\"   ROUGE-1: {rouge_scores['rouge1']:.4f}\")\nprint(f\"   ROUGE-2: {rouge_scores['rouge2']:.4f}\")\nprint(f\"   ROUGE-L: {rouge_scores['rougeL']:.4f}\")\nprint(f\"\\nüìä BLEU Score: {bleu_score:.4f}\")\n\n# Show a comparison example\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXAMPLE COMPARISON\")\nprint(\"=\"*80)\nidx = 0\nprint(f\"\\nüìù Title: {eval_samples.iloc[idx]['title']}\")\nprint(f\"\\nü•ò Ingredients: {eval_samples.iloc[idx]['ingredients'][:150]}...\")\nprint(f\"\\n‚úÖ Reference Recipe:\\n{reference_recipes[idx][:300]}...\")\nprint(f\"\\nü§ñ Generated Recipe:\\n{generated_recipes[idx][:300]}...\")\n\n\n\ndef human_evaluation_prompt(generated_recipes, num_samples=5):\n    \"\"\"Display recipes for human evaluation\"\"\"\n    print(\"=\"*80)\n    print(\"HUMAN EVALUATION\")\n    print(\"=\"*80)\n    print(\"\\nPlease rate these generated recipes on a scale of 1-5 for:\")\n    print(\"  1. Coherence: Does the recipe make logical sense?\")\n    print(\"  2. Creativity: Is the recipe interesting and creative?\")\n    print(\"  3. Completeness: Does it include all necessary steps?\")\n    print(\"=\"*80)\n    \n    samples = np.random.choice(len(generated_recipes), min(num_samples, len(generated_recipes)), replace=False)\n    \n    for i, idx in enumerate(samples):\n        print(f\"\\n{'‚îÄ'*80}\")\n        print(f\"Recipe {i+1}/{num_samples}\")\n        print(f\"{'‚îÄ'*80}\")\n        print(generated_recipes[idx])\n        print(\"\\nCoherence (1-5): __\")\n        print(\"Creativity (1-5): __\")\n        print(\"Completeness (1-5): __\")\n\n# Display evaluation prompt\nhuman_evaluation_prompt(generated_recipes, num_samples=3)\n\n\n# Save final model\nmodel.save_pretrained('./final_recipe_model')\ntokenizer.save_pretrained('./final_recipe_model')\n\nprint(\"=\"*80)\nprint(\"PROJECT SUMMARY\")\nprint(\"=\"*80)\n\nsummary = f\"\"\"\n‚úì Model: GPT-2 fine-tuned for recipe generation\n‚úì Training samples: {len(train_dataset):,}\n‚úì Validation samples: {len(val_dataset):,}\n‚úì Training epochs: {epochs}\n‚úì Best validation loss: {best_val_loss:.4f}\n\nüìä Evaluation Metrics (on {num_eval_samples} samples):\n   ‚Ä¢ ROUGE-1: {rouge_scores['rouge1']:.4f}\n   ‚Ä¢ ROUGE-2: {rouge_scores['rouge2']:.4f}\n   ‚Ä¢ ROUGE-L: {rouge_scores['rougeL']:.4f}\n   ‚Ä¢ BLEU: {bleu_score:.4f}\n\nüíæ Saved Models:\n   ‚Ä¢ ./best_recipe_model/ (best validation loss)\n   ‚Ä¢ ./final_recipe_model/ (final model)\n\n‚úÖ Deliverables Completed:\n   1. ‚úì Tokenization and dataset formatting script\n   2. ‚úì Training loop for GPT-2\n   3. ‚úì Example generations (4 examples shown)\n   4. ‚úì Quality evaluation (ROUGE, BLEU metrics)\n   5. ‚úì Human evaluation framework\n\"\"\"\n\nprint(summary)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéâ ALL TASKS COMPLETE!\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}